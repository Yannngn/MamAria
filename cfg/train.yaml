general:
  logging_file: 'logging_file_milha.log'
  log_dir:
  checkpoint_dir:
trainer:
  max_epochs: 200
  check_val_every_n_epoch: 10
  default_root_dir: ${general.checkpoint_dir}
  fast_dev_run: false
logger:
- class_name: lightning.pytorch.loggers.WandB
  params:
    save_dir: ${general.log_dir}
    project_name: 04_2023_lightning
    online: true
callbacks:
- class_name: lightning.pytorch.callbacks.early_stopping.EarlyStopping
  params:
    monitor: ${training.params.metric}
    patience: 10
    mode: ${training.params.mode}
- class_name: lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint
  params:
    monitor: ${training.params.metric}
    save_top_k: 3
    dirpath: ${general.checkpoint_dir}
    mode: ${training.params.mode}
- class_name: finetuning_scheduler.FineTuningScheduler
  params:
    gen_ft_sched_only: true
# - class_name: lightning.pytorch.callbacks.batch_size_finder.BatchSizeFinder
#   params: 
#     batch_arg_name: batch_size
- class_name: lightning.pytorch.callbacks.lr_monitor.LearningRateMonitor
  params:
    logging_interval: epoch
- class_name: lightning.pytorch.callbacks.lr_finder.LearningRateFinder
  params:
    update_attr: true
dataset:
  class_name: PhantomDCMData
  params:
    image_size: 
    - 2816
      3584
    train_img_dir: "D:/MamAria/data/train/phantom/"
    train_mask_dir: "D:/MamAria/data/train/mask/"
    val_img_dir: "D:/MamAria/data/val/phantom/"
    val_mask_dir: "D:/MamAria/data/val/mask/"
    test_img_dir: "D:/MamAria/data/test/phantom/"
    test_mask_dir: "D:/MamAria/data/test/mask/"
    calib_image_dir: "D:/MamAria/data/calib/phantom/"
    calib_mask_dir: "D:/MamAria/data/calib/phantom/"
    predictions_dir: "data/predictions/"
    submissions_dir: "data/submissions/"
    calibrations_dir: "data/calibrations/"
    confidences_dir: "data/confidences/"
    labels: 4
    label_map:
      0: 'background' 
      1: 'low_risk'
      2: 'moderate_risk'
      3: 'high_risk'
    num_workers: 12
    batch_size: 4
training:
  class_name: UNETModule
  params:
    lr: 0.001
    metric: train_loss
    seed: 1337
    debug: false
    mode: min
optimizer:
  class_name: torch.optim.AdamW
  params:
    lr: ${training.params.lr}
    weight_decay: 0.001
criterion:
  class_name: torch.nn.CrossEntropyLoss
  params: 
    weights:
    - 1
      1
      1
      1
scheduler:
  class_name: torch.optim.lr_scheduler.ReduceLROnPlateau
  step: epoch
  monitor: ${training.params.metric}
  params:
    mode: ${training.params.mode}
    factor: 0.1
    patience: 5
model:
  class_name: UNET
  params:
    min_layer_size: 64
    max_layer_size: 1024
    in_channels: 1
    labels: ${dataset.params.labels}
augmentation:
  compose:
    class_name: albumentations.Compose
  train:
    augs:
    # - class_name: albumentations.Flip
    #   params:
    #     p: 0.6
    # - class_name: albumentations.RandomBrightnessContrast
    #   params:
    #     p: 0.6
    - class_name: albumentations.pytorch.transforms.ToTensorV2
      params:
        p: 1.0
  val:
    augs:
    - class_name: albumentations.pytorch.transforms.ToTensorV2
      params:
        p: 1.0
  test:
    augs:
    - class_name: albumentations.pytorch.transforms.ToTensorV2
      params:
        p: 1.0
  calib:
    augs:
    - class_name: albumentations.pytorch.transforms.ToTensorV2
      params:
        p: 1.0